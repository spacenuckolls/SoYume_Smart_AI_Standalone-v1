import { 
  AIProvider, 
  AICapability, 
  ProviderConfig, 
  AIResponse, 
  StoryContext, 
  StoryAnalysis,
  CharacterTraits
} from '../../../shared/types/AI';
import { Character } from '../../../shared/types/Story';

export class MockLocalProvider implements AIProvider {
  name: string;
  type: 'local' = 'local';
  priority = 5; // Medium priority
  
  capabilities: AICapability[] = [
    {
      name: 'text_generation',
      description: 'Generate creative text content',
      inputTypes: ['text', 'context'],
      outputTypes: ['text'],
      offline: true
    },
    {
      name: 'basic_analysis',
      description: 'Basic text analysis and feedback',
      inputTypes: ['text'],
      outputTypes: ['analysis'],
      offline: true
    }
  ];

  private config: ProviderConfig = {};
  private initialized = false;
  private modelType: string;

  constructor(config: any) {
    this.name = config.name || 'Local AI Provider';
    this.modelType = config.modelType || 'ollama';
    this.config = config.config || {};
  }

  async initialize(config: ProviderConfig): Promise<void> {
    this.config = { ...this.config, ...config };
    
    // Simulate local model loading time
    await this.delay(1000 + Math.random() * 2000);
    
    this.initialized = true;
    console.log(`Mock Local AI (${this.modelType}) initialized with model:`, 
                this.config.modelName || 'llama2:7b');
  }

  async generateText(prompt: string, context: StoryContext): Promise<AIResponse> {
    this.ensureInitialized();
    
    const startTime = Date.now();
    
    // Simulate local processing time (slower than cloud, faster than co-writer analysis)
    await this.delay(1500 + Math.random() * 1000);
    
    const response = this.generateLocalResponse(prompt, context);
    
    return {
      content: response,
      confidence: 0.7 + Math.random() * 0.2,
      metadata: {
        model: this.config.modelName || 'llama2:7b',
        provider: this.name,
        tokensUsed: Math.floor(response.length / 4),
        responseTime: Date.now() - startTime
      },
      suggestions: this.generateBasicSuggestions(),
      alternatives: []
    };
  }

  async analyzeStory(content: string): Promise<StoryAnalysis> {
    this.ensureInitialized();
    
    // Local models provide basic analysis
    await this.delay(2000 + Math.random() * 1500);
    
    return {
      structure: {
        identifiedStructure: 'unknown',
        completedBeats: [],
        missingBeats: [],
        suggestions: ['Consider using a more structured approach to your story'],
        confidence: 0.5
      },
      characters: {
        consistencyScore: 0.6,
        voiceConsistency: 0.65,
        developmentProgress: 0.5,
        relationshipHealth: [],
        suggestions: ['Develop your characters more deeply']
      },
      pacing: {
        overallPacing: 'good',
        tensionCurve: [],
        recommendations: ['Maintain consistent pacing throughout']
      },
      consistency: {
        overallScore: 0.6,
        plotHoles: [],
        characterInconsistencies: [],
        worldBuildingIssues: []
      },
      overallScore: 0.6,
      recommendations: [
        'This is a basic analysis from a local model',
        'Consider using the Co-writer AI for more detailed feedback',
        'Focus on character development and plot structure'
      ]
    };
  }

  async generateCharacter(traits: CharacterTraits): Promise<Character> {
    this.ensureInitialized();
    
    await this.delay(1200 + Math.random() * 800);
    
    return {
      id: `char-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`,
      name: 'Generated Character',
      archetype: {
        primary: 'Protagonist',
        description: 'A main character generated by local AI',
        commonTraits: traits.personality.slice(0, 3)
      },
      traits: {
        personality: [...traits.personality, 'AI-generated'],
        motivations: [...traits.motivations, 'fulfill their destiny'],
        fears: [...traits.fears, 'unknown challenges'],
        strengths: [...traits.strengths, 'adaptability'],
        weaknesses: [...traits.weaknesses, 'inexperience'],
        quirks: [...traits.quirks, 'thoughtful pauses']
      },
      relationships: [],
      developmentArc: {
        startState: 'Uncertain about their path',
        endState: 'Confident in their abilities',
        keyMoments: [],
        completed: false
      },
      voiceProfile: {
        vocabulary: ['determined', 'hopeful', 'curious'],
        speechPatterns: ['asks questions', 'speaks thoughtfully'],
        commonPhrases: ['I wonder...', 'Perhaps we could...', 'That makes sense'],
        formalityLevel: 5,
        emotionalRange: ['curious', 'determined', 'hopeful']
      }
    };
  }

  isAvailable(): boolean {
    return this.initialized && this.isModelRunning();
  }

  async shutdown(): Promise<void> {
    this.initialized = false;
    console.log(`Mock Local AI (${this.modelType}) shutdown`);
  }

  // Local provider specific methods
  private isModelRunning(): boolean {
    // Simulate checking if local model service is running
    return Math.random() > 0.1; // 90% uptime simulation
  }

  private ensureInitialized(): void {
    if (!this.initialized) {
      throw new Error(`Local AI provider (${this.modelType}) not initialized`);
    }
  }

  private delay(ms: number): Promise<void> {
    return new Promise(resolve => setTimeout(resolve, ms));
  }

  private generateLocalResponse(prompt: string, context: StoryContext): string {
    const modelInfo = this.getModelInfo();
    
    let response = `[${this.modelType.toUpperCase()} Response]: `;
    
    // Simulate different response styles based on model type
    switch (this.modelType) {
      case 'ollama':
        response += this.generateOllamaStyle(prompt, context);
        break;
      case 'lmstudio':
        response += this.generateLMStudioStyle(prompt, context);
        break;
      case 'docker':
        response += this.generateDockerStyle(prompt, context);
        break;
      default:
        response += this.generateGenericLocalStyle(prompt, context);
    }
    
    response += `\n\n[Model: ${modelInfo.model}, Parameters: ${modelInfo.parameters}]`;
    
    return response;
  }

  private getModelInfo(): { model: string; parameters: string } {
    const model = this.config.modelName || this.getDefaultModel();
    const params = this.formatParameters();
    
    return { model, parameters: params };
  }

  private getDefaultModel(): string {
    switch (this.modelType) {
      case 'ollama': return 'llama2:7b';
      case 'lmstudio': return 'mistral-7b-instruct';
      case 'docker': return 'local-llm:latest';
      default: return 'unknown-model';
    }
  }

  private formatParameters(): string {
    const temp = this.config.temperature || 0.7;
    const tokens = this.config.maxTokens || 2048;
    return `temp=${temp}, max_tokens=${tokens}`;
  }

  private generateOllamaStyle(prompt: string, context: StoryContext): string {
    return `Based on your prompt about "${prompt.substring(0, 50)}...", I can help you develop this idea further.

${this.getGenreSpecificAdvice(context.genre)}

Here's my suggestion: ${this.generateCreativeContent(prompt, context)}

This response is generated locally using Ollama, ensuring your creative work stays private on your machine.`;
  }

  private generateLMStudioStyle(prompt: string, context: StoryContext): string {
    return `Analyzing your creative prompt through LM Studio...

**Creative Analysis:**
${this.generateCreativeContent(prompt, context)}

**Context Considerations:**
- Genre: ${context.genre.join(', ') || 'General'}
- Target Audience: ${context.targetAudience || 'General'}
- Character Count: ${context.characters.length}

**Suggestion:**
${this.getGenreSpecificAdvice(context.genre)}

*Generated locally with LM Studio for maximum privacy and control.*`;
  }

  private generateDockerStyle(prompt: string, context: StoryContext): string {
    return `[DOCKER CONTAINER: local-ai-service]

Processing creative writing request...
Input: "${prompt.substring(0, 40)}..."
Context: ${context.genre.length} genres, ${context.characters.length} characters

Output:
${this.generateCreativeContent(prompt, context)}

Additional Notes:
${this.getGenreSpecificAdvice(context.genre)}

[Container Status: Running | Memory Usage: 2.1GB | Response Time: ${Math.floor(Math.random() * 2000 + 500)}ms]`;
  }

  private generateGenericLocalStyle(prompt: string, context: StoryContext): string {
    return `Processing your creative writing request locally...

${this.generateCreativeContent(prompt, context)}

Context Analysis:
- Working with ${context.genre.join(' and ') || 'general'} content
- ${context.characters.length} characters in scope
- Target audience: ${context.targetAudience || 'General'}

${this.getGenreSpecificAdvice(context.genre)}

This response was generated entirely on your local machine, ensuring complete privacy.`;
  }

  private generateCreativeContent(prompt: string, context: StoryContext): string {
    const templates = [
      `Your idea about "${prompt}" has strong potential. Consider developing the emotional core of this concept by exploring what drives your characters' actions and decisions.`,
      
      `The premise "${prompt}" offers interesting possibilities. Think about the conflict that will drive your story forward and how it challenges your protagonist.`,
      
      `"${prompt}" is a compelling starting point. Focus on creating authentic character voices and ensuring each scene advances either plot or character development.`,
      
      `Your concept around "${prompt}" could benefit from deeper world-building. Consider the rules and limitations of your story world to create believable conflicts.`
    ];
    
    return templates[Math.floor(Math.random() * templates.length)];
  }

  private getGenreSpecificAdvice(genres: string[]): string {
    if (genres.includes('fantasy')) {
      return 'For fantasy stories, establish clear magic system rules and ensure consistent world-building throughout.';
    }
    if (genres.includes('romance')) {
      return 'In romance, focus on the emotional journey and character chemistry. Show, don\'t tell, the growing connection.';
    }
    if (genres.includes('mystery')) {
      return 'Mystery stories require careful clue placement and red herrings. Ensure fair play with your readers.';
    }
    if (genres.includes('sci-fi')) {
      return 'Science fiction should ground fantastical elements in believable science and explore their implications.';
    }
    
    return 'Focus on strong character development and clear story structure regardless of genre.';
  }

  private generateBasicSuggestions(): string[] {
    const suggestions = [
      'Consider adding more descriptive details',
      'Develop character motivations further',
      'Think about the pacing of this section',
      'Ensure dialogue sounds natural',
      'Add sensory details to enhance immersion'
    ];
    
    return suggestions.slice(0, 2 + Math.floor(Math.random() * 2));
  }

  // Simulate different local AI provider capabilities
  getProviderSpecificInfo(): any {
    switch (this.modelType) {
      case 'ollama':
        return {
          installedModels: ['llama2:7b', 'mistral:7b', 'codellama:7b'],
          isRunning: this.isModelRunning(),
          version: '0.1.32',
          memoryUsage: '2.1GB'
        };
      case 'lmstudio':
        return {
          availableModels: ['mistral-7b-instruct', 'llama-2-7b-chat', 'codellama-7b'],
          currentModel: this.config.modelName || 'mistral-7b-instruct',
          serverPort: 1234,
          status: 'running'
        };
      case 'docker':
        return {
          containerName: 'local-ai-service',
          image: 'local-llm:latest',
          status: 'running',
          ports: ['8080:8080'],
          memoryLimit: '4GB'
        };
      default:
        return {
          type: 'generic',
          status: 'running'
        };
    }
  }
}