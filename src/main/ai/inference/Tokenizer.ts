// Tokenizer interface and implementations for text processing\n\nexport interface TokenizerConfig {\n  vocabSize: number;\n  maxLength: number;\n  padToken: string;\n  unknownToken: string;\n  bosToken?: string;\n  eosToken?: string;\n  specialTokens?: Record<string, number>;\n}\n\nexport interface TokenizationResult {\n  tokens: number[];\n  tokenStrings: string[];\n  attentionMask: number[];\n  specialTokenMask: number[];\n}\n\nexport interface DecodingOptions {\n  skipSpecialTokens?: boolean;\n  cleanUpTokenizationSpaces?: boolean;\n}\n\n// Base tokenizer interface\nexport abstract class BaseTokenizer {\n  protected config: TokenizerConfig;\n  protected vocab: Map<string, number> = new Map();\n  protected reverseVocab: Map<number, string> = new Map();\n  protected specialTokens: Map<string, number> = new Map();\n\n  constructor(config: TokenizerConfig) {\n    this.config = config;\n    this.initializeSpecialTokens();\n  }\n\n  abstract encode(text: string, addSpecialTokens?: boolean): TokenizationResult;\n  abstract decode(tokens: number[], options?: DecodingOptions): string;\n  abstract getTokenId(token: string): number;\n  abstract getToken(tokenId: number): string;\n\n  protected initializeSpecialTokens(): void {\n    this.specialTokens.set(this.config.padToken, 0);\n    this.specialTokens.set(this.config.unknownToken, 1);\n    \n    if (this.config.bosToken) {\n      this.specialTokens.set(this.config.bosToken, 2);\n    }\n    \n    if (this.config.eosToken) {\n      this.specialTokens.set(this.config.eosToken, 3);\n    }\n\n    if (this.config.specialTokens) {\n      for (const [token, id] of Object.entries(this.config.specialTokens)) {\n        this.specialTokens.set(token, id);\n      }\n    }\n  }\n\n  getVocabSize(): number {\n    return this.config.vocabSize;\n  }\n\n  getSpecialTokens(): Map<string, number> {\n    return new Map(this.specialTokens);\n  }\n\n  isSpecialToken(token: string): boolean {\n    return this.specialTokens.has(token);\n  }\n\n  padSequence(tokens: number[], maxLength?: number): number[] {\n    const targetLength = maxLength || this.config.maxLength;\n    const padTokenId = this.specialTokens.get(this.config.padToken) || 0;\n    \n    if (tokens.length >= targetLength) {\n      return tokens.slice(0, targetLength);\n    }\n    \n    const padded = [...tokens];\n    while (padded.length < targetLength) {\n      padded.push(padTokenId);\n    }\n    \n    return padded;\n  }\n\n  createAttentionMask(tokens: number[]): number[] {\n    const padTokenId = this.specialTokens.get(this.config.padToken) || 0;\n    return tokens.map(token => token === padTokenId ? 0 : 1);\n  }\n}\n\n// Simple word-level tokenizer for basic functionality\nexport class WordTokenizer extends BaseTokenizer {\n  private wordToId: Map<string, number> = new Map();\n  private idToWord: Map<number, string> = new Map();\n  private nextId = 100; // Start after special tokens\n\n  constructor(config: TokenizerConfig, vocabulary?: string[]) {\n    super(config);\n    \n    if (vocabulary) {\n      this.buildVocabulary(vocabulary);\n    }\n  }\n\n  encode(text: string, addSpecialTokens = true): TokenizationResult {\n    // Basic preprocessing\n    const cleanText = this.preprocessText(text);\n    const words = this.tokenizeText(cleanText);\n    \n    let tokens: number[] = [];\n    let tokenStrings: string[] = [];\n    \n    // Add BOS token if requested\n    if (addSpecialTokens && this.config.bosToken) {\n      const bosId = this.specialTokens.get(this.config.bosToken)!;\n      tokens.push(bosId);\n      tokenStrings.push(this.config.bosToken);\n    }\n    \n    // Convert words to token IDs\n    for (const word of words) {\n      const tokenId = this.getTokenId(word);\n      tokens.push(tokenId);\n      tokenStrings.push(word);\n    }\n    \n    // Add EOS token if requested\n    if (addSpecialTokens && this.config.eosToken) {\n      const eosId = this.specialTokens.get(this.config.eosToken)!;\n      tokens.push(eosId);\n      tokenStrings.push(this.config.eosToken);\n    }\n    \n    // Truncate if too long\n    if (tokens.length > this.config.maxLength) {\n      tokens = tokens.slice(0, this.config.maxLength);\n      tokenStrings = tokenStrings.slice(0, this.config.maxLength);\n    }\n    \n    const attentionMask = this.createAttentionMask(tokens);\n    const specialTokenMask = tokens.map(token => \n      Array.from(this.specialTokens.values()).includes(token) ? 1 : 0\n    );\n    \n    return {\n      tokens,\n      tokenStrings,\n      attentionMask,\n      specialTokenMask\n    };\n  }\n\n  decode(tokens: number[], options: DecodingOptions = {}): string {\n    const { skipSpecialTokens = true, cleanUpTokenizationSpaces = true } = options;\n    \n    let words: string[] = [];\n    \n    for (const tokenId of tokens) {\n      const token = this.getToken(tokenId);\n      \n      if (skipSpecialTokens && this.isSpecialTokenById(tokenId)) {\n        continue;\n      }\n      \n      words.push(token);\n    }\n    \n    let result = words.join(' ');\n    \n    if (cleanUpTokenizationSpaces) {\n      result = this.cleanupSpaces(result);\n    }\n    \n    return result;\n  }\n\n  getTokenId(token: string): number {\n    // Check special tokens first\n    if (this.specialTokens.has(token)) {\n      return this.specialTokens.get(token)!;\n    }\n    \n    // Check vocabulary\n    if (this.wordToId.has(token)) {\n      return this.wordToId.get(token)!;\n    }\n    \n    // Add to vocabulary if space available\n    if (this.wordToId.size < this.config.vocabSize - this.specialTokens.size) {\n      const id = this.nextId++;\n      this.wordToId.set(token, id);\n      this.idToWord.set(id, token);\n      return id;\n    }\n    \n    // Return unknown token ID\n    return this.specialTokens.get(this.config.unknownToken) || 1;\n  }\n\n  getToken(tokenId: number): string {\n    // Check special tokens\n    for (const [token, id] of this.specialTokens) {\n      if (id === tokenId) {\n        return token;\n      }\n    }\n    \n    // Check vocabulary\n    if (this.idToWord.has(tokenId)) {\n      return this.idToWord.get(tokenId)!;\n    }\n    \n    return this.config.unknownToken;\n  }\n\n  private buildVocabulary(vocabulary: string[]): void {\n    for (let i = 0; i < vocabulary.length && i < this.config.vocabSize; i++) {\n      const word = vocabulary[i];\n      const id = this.nextId++;\n      this.wordToId.set(word, id);\n      this.idToWord.set(id, word);\n    }\n  }\n\n  private preprocessText(text: string): string {\n    return text\n      .toLowerCase()\n      .replace(/[^a-zA-Z0-9\\s.,!?;:\"'()\\-]/g, '') // Keep basic punctuation\n      .replace(/\\s+/g, ' ')\n      .trim();\n  }\n\n  private tokenizeText(text: string): string[] {\n    // Simple word tokenization with punctuation handling\n    return text\n      .split(/\\s+/)\n      .flatMap(word => this.splitPunctuation(word))\n      .filter(token => token.length > 0);\n  }\n\n  private splitPunctuation(word: string): string[] {\n    const tokens: string[] = [];\n    let current = '';\n    \n    for (const char of word) {\n      if (/[.,!?;:\"'()\\-]/.test(char)) {\n        if (current) {\n          tokens.push(current);\n          current = '';\n        }\n        tokens.push(char);\n      } else {\n        current += char;\n      }\n    }\n    \n    if (current) {\n      tokens.push(current);\n    }\n    \n    return tokens;\n  }\n\n  private isSpecialTokenById(tokenId: number): boolean {\n    return Array.from(this.specialTokens.values()).includes(tokenId);\n  }\n\n  private cleanupSpaces(text: string): string {\n    return text\n      .replace(/\\s+([.,!?;:])/g, '$1') // Remove spaces before punctuation\n      .replace(/\\(\\s+/g, '(') // Remove spaces after opening parentheses\n      .replace(/\\s+\\)/g, ')') // Remove spaces before closing parentheses\n      .replace(/\\s+/g, ' ') // Normalize multiple spaces\n      .trim();\n  }\n}\n\n// BPE (Byte Pair Encoding) tokenizer for more advanced tokenization\nexport class BPETokenizer extends BaseTokenizer {\n  private merges: Map<string, number> = new Map();\n  private bpeRanks: Map<string, number> = new Map();\n  private cache: Map<string, string[]> = new Map();\n\n  constructor(config: TokenizerConfig, merges?: Array<[string, string]>, vocab?: Map<string, number>) {\n    super(config);\n    \n    if (merges) {\n      this.initializeMerges(merges);\n    }\n    \n    if (vocab) {\n      this.vocab = new Map(vocab);\n      this.reverseVocab = new Map(Array.from(vocab.entries()).map(([k, v]) => [v, k]));\n    }\n  }\n\n  encode(text: string, addSpecialTokens = true): TokenizationResult {\n    const preprocessed = this.preprocessText(text);\n    let tokens: string[] = [];\n    \n    // Add BOS token if requested\n    if (addSpecialTokens && this.config.bosToken) {\n      tokens.push(this.config.bosToken);\n    }\n    \n    // Apply BPE encoding\n    const words = preprocessed.split(/\\s+/);\n    for (const word of words) {\n      if (word.length > 0) {\n        const bpeTokens = this.bpeEncode(word);\n        tokens.push(...bpeTokens);\n      }\n    }\n    \n    // Add EOS token if requested\n    if (addSpecialTokens && this.config.eosToken) {\n      tokens.push(this.config.eosToken);\n    }\n    \n    // Convert to token IDs\n    const tokenIds = tokens.map(token => this.getTokenId(token));\n    \n    // Truncate if necessary\n    const maxLen = Math.min(tokenIds.length, this.config.maxLength);\n    const finalTokens = tokenIds.slice(0, maxLen);\n    const finalTokenStrings = tokens.slice(0, maxLen);\n    \n    const attentionMask = this.createAttentionMask(finalTokens);\n    const specialTokenMask = finalTokens.map(token => \n      Array.from(this.specialTokens.values()).includes(token) ? 1 : 0\n    );\n    \n    return {\n      tokens: finalTokens,\n      tokenStrings: finalTokenStrings,\n      attentionMask,\n      specialTokenMask\n    };\n  }\n\n  decode(tokens: number[], options: DecodingOptions = {}): string {\n    const { skipSpecialTokens = true, cleanUpTokenizationSpaces = true } = options;\n    \n    let tokenStrings: string[] = [];\n    \n    for (const tokenId of tokens) {\n      const token = this.getToken(tokenId);\n      \n      if (skipSpecialTokens && this.isSpecialTokenById(tokenId)) {\n        continue;\n      }\n      \n      tokenStrings.push(token);\n    }\n    \n    let result = tokenStrings.join('');\n    \n    if (cleanUpTokenizationSpaces) {\n      result = this.cleanupBPESpaces(result);\n    }\n    \n    return result;\n  }\n\n  getTokenId(token: string): number {\n    if (this.vocab.has(token)) {\n      return this.vocab.get(token)!;\n    }\n    \n    return this.specialTokens.get(this.config.unknownToken) || 1;\n  }\n\n  getToken(tokenId: number): string {\n    if (this.reverseVocab.has(tokenId)) {\n      return this.reverseVocab.get(tokenId)!;\n    }\n    \n    return this.config.unknownToken;\n  }\n\n  private initializeMerges(merges: Array<[string, string]>): void {\n    for (let i = 0; i < merges.length; i++) {\n      const [first, second] = merges[i];\n      const pair = `${first} ${second}`;\n      this.bpeRanks.set(pair, i);\n    }\n  }\n\n  private bpeEncode(word: string): string[] {\n    if (this.cache.has(word)) {\n      return this.cache.get(word)!;\n    }\n    \n    let pairs = this.getPairs(word.split(''));\n    \n    if (pairs.length === 0) {\n      this.cache.set(word, [word]);\n      return [word];\n    }\n    \n    while (true) {\n      const bigram = this.getMinPair(pairs);\n      if (!this.bpeRanks.has(bigram)) {\n        break;\n      }\n      \n      const [first, second] = bigram.split(' ');\n      let newWord: string[] = [];\n      let i = 0;\n      \n      while (i < word.length) {\n        const j = word.indexOf(first, i);\n        if (j === -1) {\n          newWord.push(...word.slice(i).split(''));\n          break;\n        }\n        \n        newWord.push(...word.slice(i, j).split(''));\n        i = j;\n        \n        if (word.slice(i, i + first.length) === first && \n            i + first.length < word.length && \n            word.slice(i + first.length, i + first.length + second.length) === second) {\n          newWord.push(first + second);\n          i += first.length + second.length;\n        } else {\n          newWord.push(word[i]);\n          i++;\n        }\n      }\n      \n      word = newWord.join('');\n      if (newWord.length === 1) {\n        break;\n      }\n      \n      pairs = this.getPairs(newWord);\n    }\n    \n    const result = word.split('');\n    this.cache.set(word, result);\n    return result;\n  }\n\n  private getPairs(word: string[]): string[] {\n    const pairs: string[] = [];\n    for (let i = 0; i < word.length - 1; i++) {\n      pairs.push(`${word[i]} ${word[i + 1]}`);\n    }\n    return pairs;\n  }\n\n  private getMinPair(pairs: string[]): string {\n    let minPair = pairs[0];\n    let minRank = this.bpeRanks.get(minPair) || Infinity;\n    \n    for (const pair of pairs) {\n      const rank = this.bpeRanks.get(pair) || Infinity;\n      if (rank < minRank) {\n        minRank = rank;\n        minPair = pair;\n      }\n    }\n    \n    return minPair;\n  }\n\n  private preprocessText(text: string): string {\n    // BPE typically works with raw text, minimal preprocessing\n    return text.replace(/\\s+/g, ' ').trim();\n  }\n\n  private isSpecialTokenById(tokenId: number): boolean {\n    return Array.from(this.specialTokens.values()).includes(tokenId);\n  }\n\n  private cleanupBPESpaces(text: string): string {\n    // BPE tokens often have special markers for spaces\n    return text\n      .replace(/Ġ/g, ' ') // GPT-style space marker\n      .replace(/▁/g, ' ') // SentencePiece space marker\n      .replace(/\\s+/g, ' ')\n      .trim();\n  }\n}\n\n// Factory function to create tokenizers\nexport function createTokenizer(\n  type: 'word' | 'bpe',\n  config: TokenizerConfig,\n  options?: {\n    vocabulary?: string[] | Map<string, number>;\n    merges?: Array<[string, string]>;\n  }\n): BaseTokenizer {\n  switch (type) {\n    case 'word':\n      return new WordTokenizer(config, options?.vocabulary as string[]);\n    case 'bpe':\n      return new BPETokenizer(\n        config,\n        options?.merges,\n        options?.vocabulary as Map<string, number>\n      );\n    default:\n      throw new Error(`Unsupported tokenizer type: ${type}`);\n  }\n}\n\n// Utility functions\nexport function getDefaultTokenizerConfig(): TokenizerConfig {\n  return {\n    vocabSize: 50000,\n    maxLength: 2048,\n    padToken: '<|pad|>',\n    unknownToken: '<|unk|>',\n    bosToken: '<|startoftext|>',\n    eosToken: '<|endoftext|>'\n  };\n}\n\nexport function estimateTokenCount(text: string, averageTokenLength = 4): number {\n  // Rough estimation: average English token is about 4 characters\n  return Math.ceil(text.length / averageTokenLength);\n}\n\nexport function truncateToTokenLimit(text: string, maxTokens: number, averageTokenLength = 4): string {\n  const maxChars = maxTokens * averageTokenLength;\n  if (text.length <= maxChars) {\n    return text;\n  }\n  \n  // Try to truncate at word boundary\n  const truncated = text.slice(0, maxChars);\n  const lastSpace = truncated.lastIndexOf(' ');\n  \n  if (lastSpace > maxChars * 0.8) {\n    return truncated.slice(0, lastSpace);\n  }\n  \n  return truncated;\n}"